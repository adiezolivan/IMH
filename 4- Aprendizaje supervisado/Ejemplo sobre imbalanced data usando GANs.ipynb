{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MÓDULO 4: LA CIENCIA DE DATOS Y LOS MODELOS DE ANALÍTICA PREDICTIVA EN LA INDUSTRIA 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Aprendizaje supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo sobre imbalanced data\n",
    "\n",
    "En este ejemplo se pretende mostrar una manera muy interesante de generar datos sintéticos cuando el dataset está desbalanceado. En concreto, estaríamos hablando de redes GAN, o redes generativas antagónicas, y adversarial learning.\n",
    "\n",
    "La idea consiste en utilizar 2 tipos de redes: una generativa y otra discriminadora. La discriminadora ayuda a la generadora a ofrecer resultados más certeros y tan similares a los datos reales que, cuando convergen, no se pueden llegar a diferenciar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Señales unidimensionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente se utilizan para la generación de imágenes sintéticas, pero en este caso nos vamos a centrar en señales unidimensionales, \"fácilmente\" extrapolables a señales y patrones temporales. Vamos a verlo...\n",
    "\n",
    "En primera instancia, necesitamos importar funcionalidad del paquete 'keras', ya que las GAN se basan en redes deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from random import seed\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la red discriminadora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(n_inputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la red generadora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim, \n",
    "                     n_outputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "    model.add(Dense(n_outputs, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definimos el modelo que combina la red generadora y discriminadora, para ir actualizando la primera de las dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(generator, \n",
    "               discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos 'n' datos reales con sus correspondientes etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(n):\n",
    "    # generate inputs in [-0.5, 0.5]\n",
    "    X1 = rand(n) - 0.5\n",
    "    # generate outputs X^2\n",
    "    X2 = X1 * X1\n",
    "    # stack arrays\n",
    "    X1 = X1.reshape(n, 1)\n",
    "    X2 = X2.reshape(n, 1)\n",
    "    X = hstack((X1, X2))\n",
    "    # generate class labels\n",
    "    y = ones((n, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una serie de puntos en el espacio \"latente\" o implícito, no directamente observable, como entrada del modelo generativo. Típicamente se trata de una hiperesfera con cada variable proyectada desde una distribución Gausiana con media 0 y desviación 1. A través del entrenamiento el modelo generador aprende a mapear los puntos en este espacio \"latente\" con señales unidimensionales específicas de manera incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, \n",
    "                           n):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar la red generativa para generar 'n' señales sintéticas, con las correspondientes etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, \n",
    "                          latent_dim, \n",
    "                          n):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = zeros((n, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasarle los datos generados a la red discriminadora y plotear los puntos reales y los sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(epoch, \n",
    "                          generator, \n",
    "                          discriminator, \n",
    "                          latent_dim, \n",
    "                          n=100):\n",
    "    # prepare real samples\n",
    "    x_real, y_real = generate_real_samples(n)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print(epoch, acc_real, acc_fake)\n",
    "    # scatter plot real and fake data points\n",
    "    plt.figure()\n",
    "    plt.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "    plt.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "    plt.show()\n",
    "    return x_real, y_real, x_fake, y_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar los modelos generador y discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, \n",
    "          d_model, \n",
    "          gan_model, \n",
    "          latent_dim, \n",
    "          n_epochs=10000, \n",
    "          n_batch=128, \n",
    "          n_eval=2000):\n",
    "    # determine half the size of one batch, for updating the discriminator\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # prepare real samples\n",
    "        x_real, y_real = generate_real_samples(half_batch)\n",
    "        # prepare fake examples\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        # update discriminator\n",
    "        d_model.train_on_batch(x_real, y_real)\n",
    "        d_model.train_on_batch(x_fake, y_fake)\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        gan_model.train_on_batch(x_gan, y_gan)\n",
    "        # evaluate the model every n_eval epochs\n",
    "        if (i+1) % n_eval == 0:\n",
    "            x_real, y_real, x_fake, y_fake = summarize_performance(i, g_model, d_model, latent_dim)\n",
    "    return x_real, y_real, x_fake, y_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos y vemos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# train model\n",
    "x_real, y_real, x_fake, y_fake = train(generator, discriminator, gan_model, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Real: ' + str(np.unique(np.round(discriminator.predict(x_real)).reshape(1,-1)[0], return_counts=True)))\n",
    "print('Sintético: ' + str(np.unique(np.round(discriminator.predict(x_fake)).reshape(1,-1)[0], return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que el proceso de entrenamiento es relativamente inestable. Primero se muestra la iteración (epochs), en segunda instancia la precisión del modelo discriminador al clasificar casos reales y, finalmente, la precisión conseguida al utilizar señales sintéticas.\n",
    "\n",
    "La red discriminadora se mantienen confusa con los casos reales y con los datos sintéticos. Queda trabajo por hacer en esta línea..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
